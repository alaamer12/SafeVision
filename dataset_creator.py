# -*- coding: utf-8 -*-
"""dataset_creator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1he4OWlUt3t1PeDkHetrx8ywFjgman6-4

# Now EveryThing is ready
"""

!pip install tqdm requests aiohttp aiofiles pillow numpy opencv-python aiofiles colorama ultralytics

import os
import pandas as pd
from tqdm.auto import tqdm
from datetime import datetime
import threading
import queue
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter
import threading
import asyncio
import aiohttp
import aiofiles
import io
from PIL import Image
import numpy as np
import random
import platform
import sys
import contextlib
import hashlib
# noinspection PyCompatibility
import imghdr
import io
import logging
import platform
import sys
import time
from typing import Set, List, Tuple
from tqdm.auto import tqdm
import requests
from PIL import Image
from urllib3.util.retry import Retry
import cv2
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.preprocessing import image
from sklearn.metrics.pairwise import cosine_similarity
import shutil
from ultralytics import YOLO
import sys
from natsort import natsorted

DATASET_IMAGES = 100000
UNDETERMINED_DATASET_IMAGES = 150000

RAW_DATA_DIR = 'data'
BASE_DIR = 'dataset'
NSFW_IMAGES_DIR = "dataset/NSFW/images"
SFW_IMAGES_DIR = "dataset/SFW/images"

TRAIN_SIZE = 0.8
VAL_SIZE = 0.1
TEST_SIZE = 0.1
RANDOM_STATE = 42

DATA_SPLIT_OUTPUT_DIR = "dataset_split"
DATA_AUGMENTATION_OUTPUT_DIR = "dataset_augmented"

MODEL_DIR = 'model'
EPOCHS = 10
IMGSIZE = 640
BATCH_SIZE = 16

init()

def clear_console():
    """Clear console output in both regular Python and Jupyter/Colab environments"""
    try:
        # Try to clear IPython/Jupyter output first
        from IPython import get_ipython
        get_ipython().run_line_magic('clear', '')
        get_ipython().run_line_magic('reset_selective', '-f tqdm')
    except:
        # Fall back to regular console clearing
        if platform.system() == "Windows":
            os.system('cls')
        else:
            os.system('clear')

class TqdmToLogger(io.StringIO):
    """Output stream for TQDM which will output to logger module instead of stdout"""
    logger = None
    level = None
    buf = ''

    def __init__(self, logger, level=None):
        super(TqdmToLogger, self).__init__()
        self.logger = logger
        self.level = level or logging.INFO

    def write(self, buf):
        self.buf = buf.strip('\r\n\t ')

    def flush(self):
        if self.buf:
            self.logger.log(self.level, self.buf)

# Configure logging with both file and custom stream handler
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download_progress.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Create custom TQDM output stream
tqdm_out = TqdmToLogger(logger, logging.INFO)

class OpenImagesDownloader:
    def __init__(self, target_dir=SFW_IMAGES_DIR, max_images=100000, chunk_size=1024*1024):
        self.target_dir = target_dir
        self.max_images = max_images
        self.chunk_size = chunk_size
        self.image_urls = queue.Queue()
        self.download_lock = threading.Lock()
        self.session = self._create_session()
        self.successful_downloads = set()
        self.failed_urls = set()
        self.download_counter = 0
        self.stop_event = threading.Event()
        self.url_cache = set()
        self.semaphore = threading.Semaphore(100)
        os.makedirs(target_dir, exist_ok=True)

    def _create_session(self):
        """Create an optimized requests session"""
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        # Increased pool size for better performance
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=100,
            pool_maxsize=100,
            pool_block=False
        )
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session

    async def _download_with_aiohttp(self, url, filename, session):
        """Asynchronous download using aiohttp"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            async with session.get(url, timeout=timeout) as response:
                if response.status == 200:
                    async with aiofiles.open(filename, 'wb') as f:
                        while True:
                            chunk = await response.content.read(self.chunk_size)
                            if not chunk:
                                break
                            await f.write(chunk)
                    return True
        except Exception as e:
            logger.error(f"Async download error for {url}: {str(e)}")
        return False

    def is_valid_image(self, file_path):
        """Validate image file"""
        try:
            with Image.open(file_path) as img:
                img.verify()
                return True
        except:
            return False

    async def download_csv_files(self):
        """Download necessary CSV files from Open Images Dataset"""
        files = {
            'train': [
                'https://storage.googleapis.com/cvdf-datasets/oid/open-images-dataset-train0.tsv',
                'https://storage.googleapis.com/cvdf-datasets/oid/open-images-dataset-train1.tsv',
                'https://storage.googleapis.com/cvdf-datasets/oid/open-images-dataset-train2.tsv'
            ],
            'validation': [
                'https://storage.googleapis.com/cvdf-datasets/oid/open-images-dataset-validation.tsv'
            ],
            'test': [
                'https://storage.googleapis.com/cvdf-datasets/oid/open-images-dataset-test.tsv'
            ]
        }

        for split, urls in files.items():
            for url in urls:
                filename = f"{split}_{url.split('/')[-1]}"
                local_path = os.path.join(self.target_dir, filename)

                if not os.path.exists(local_path):
                    logger.info(f"Downloading {filename}...")
                    try:
                        headers = {
                            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                            'Accept-Language': 'en-US,en;q=0.5',
                            'Connection': 'keep-alive',
                        }

                        # Try primary URL
                        response = self.session.get(url, headers=headers, timeout=30, stream=True)

                        if response.status_code == 403:
                            # Try backup URL
                            backup_url = f"https://open-images-dataset.s3.amazonaws.com/{filename}"
                            logger.info(f"Retrying with backup URL: {backup_url}")
                            response = self.session.get(backup_url, headers=headers, timeout=30, stream=True)

                        response.raise_for_status()

                        # Get total file size for progress bar
                        total_size = int(response.headers.get('content-length', 0))

                        # Stream download with progress bar
                        with open(local_path, 'wb') as f, tqdm(
                            desc=f"Downloading {filename}",
                            total=total_size,
                            unit='iB',
                            unit_scale=True,
                            unit_divisor=1024,
                        ) as pbar:
                            for chunk in response.iter_content(chunk_size=8192):
                                if chunk:
                                    size = f.write(chunk)
                                    pbar.update(size)

                        # Verify file size
                        if os.path.getsize(local_path) != total_size and total_size > 0:
                            raise Exception("Downloaded file size does not match expected size")

                        logger.info(f"Successfully downloaded {filename}")

                    except Exception as e:
                        logger.error(f"Failed to download {filename}: {str(e)}")
                        if os.path.exists(local_path):
                            os.remove(local_path)  # Clean up partial download
                        if split == 'train':  # Only raise for train set
                            raise
                        continue
                else:
                    logger.info(f"Using existing file: {filename}")

    def read_image_urls_from_tsv(self, tsv_file):
        """Read image URLs from TSV file"""
        try:
            # First try reading with pandas
            df = pd.read_csv(tsv_file, sep='\t', on_bad_lines='skip')
            logger.info(f"Columns in {os.path.basename(tsv_file)}: {list(df.columns)}")

            # Try different possible column names for URLs
            url_columns = ['OriginalURL', 'URL', 'original_url', 'url', 'ImageID']

            # Find the first matching column
            url_column = None
            for col in url_columns:
                if col in df.columns:
                    url_column = col
                    break

            if url_column is None:
                # If no standard column found, try reading raw
                with open(tsv_file, 'r', encoding='utf-8') as f:
                    # Read first few lines to analyze
                    header = f.readline().strip()
                    first_line = f.readline().strip() if f.readline() else ""

                    # If we have data, try to extract URL
                    if first_line:
                        # Split by tab and try to find a URL-like string
                        parts = first_line.split('\t')
                        for i, part in enumerate(parts):
                            if part.startswith(('http://', 'https://')):
                                # Found URL column, now read all URLs
                                urls = []
                                f.seek(0)  # Go back to start
                                next(f)  # Skip header
                                for line in f:
                                    try:
                                        url = line.strip().split('\t')[i]
                                        if url.startswith(('http://', 'https://')):
                                            urls.append(url)
                                    except:
                                        continue
                                return urls

                # If we get here, we couldn't find URLs
                logger.error(f"Available columns in {os.path.basename(tsv_file)}: {list(df.columns)}")
                logger.error(f"Error reading {os.path.basename(tsv_file)}: 'URL column not found'")
                logger.error(f"File path: {tsv_file}")
                return []

            # Extract URLs from the found column
            urls = []
            if url_column == 'ImageID':
                # Special handling for ImageID - construct URL
                base_url = "https://storage.googleapis.com/openimages/v6/images"
                urls = [f"{base_url}/{img_id}.jpg" for img_id in df[url_column] if pd.notna(img_id)]
            else:
                # Regular URL column
                urls = [url for url in df[url_column] if pd.notna(url)]

            logger.info(f"Found {len(urls)} URLs in {os.path.basename(tsv_file)}")
            return urls

        except Exception as e:
            logger.error(f"Error reading {os.path.basename(tsv_file)}: {str(e)}")
            logger.error(f"File path: {tsv_file}")
            return []

    def load_image_urls(self):
        """Load and filter image URLs from CSV files"""
        logger.info("Loading image URLs...")

        # List of TSV files in the target directory
        tsv_files = [f for f in os.listdir(self.target_dir) if f.endswith('.tsv')]

        if not tsv_files:
            raise FileNotFoundError("No TSV files found. Please run download_csv_files() first.")

        urls = []
        for tsv_file in tsv_files:
            file_path = os.path.join(self.target_dir, tsv_file)
            urls.extend(self.read_image_urls_from_tsv(file_path))

        # Shuffle URLs for variety
        random.shuffle(urls)

        # Take only the required number of URLs
        urls = urls[:self.max_images]

        # Add URLs to the queue
        for url in urls:
            self.image_urls.put(url)

        logger.info(f"Loaded {self.image_urls.qsize()} URLs")

    async def download_images_async(self):
        """Asynchronous image download with proper progress tracking"""
        logger.info("Starting image download...")

        # Progress bars setup
        progress_format = "{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        main_pbar = tqdm(
            total=self.max_images,
            desc="Total Progress",
            unit="img",
            position=0,
            bar_format=progress_format,
            file=tqdm_out,
            dynamic_ncols=True
        )
        download_pbar = tqdm(
            total=self.max_images,
            desc="Downloaded",
            unit="img",
            position=1,
            bar_format=progress_format,
            file=tqdm_out,
            dynamic_ncols=True
        )

        try:
            # Create task queue
            tasks = []
            max_concurrent = min(100, os.cpu_count() * 4)

            async with aiohttp.ClientSession() as session:
                while not self.stop_event.is_set():
                    try:
                        url = self.image_urls.get_nowait()
                    except queue.Empty:
                        if not tasks:  # No more URLs and no pending tasks
                            break
                        else:
                            # Wait for some tasks to complete
                            done, tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
                            continue

                    # Skip if already downloaded or reached max
                    with self.download_lock:
                        if url in self.url_cache or len(self.successful_downloads) >= self.max_images:
                            self.image_urls.task_done()
                            continue
                        self.url_cache.add(url)

                    # Generate filename
                    with self.download_lock:
                        self.download_counter += 1
                        filename = os.path.join(self.target_dir, f"image{self.download_counter}.jpg")

                    # Create download task
                    if len(tasks) >= max_concurrent:
                        # Wait for a task to complete before adding more
                        done, tasks = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
                        tasks = list(tasks)

                    task = asyncio.create_task(self._download_with_aiohttp(url, filename, session))
                    tasks.append(task)

                    # Update progress
                    current_count = len(self.successful_downloads)
                    failed_count = len(self.failed_urls)
                    total_processed = current_count + failed_count

                    if total_processed > 0:
                        success_rate = (current_count / total_processed) * 100
                        speed = current_count / (time.time() - self.start_time)

                        download_pbar.set_postfix({
                            "failed": failed_count,
                            "success_rate": f"{success_rate:.1f}%",
                            "speed": f"{speed:.1f} img/s"
                        })

                    main_pbar.update(1)
                    download_pbar.update(1)

                    # Check if we've reached the target
                    if current_count >= self.max_images:
                        break

                    await asyncio.sleep(0.01)  # Prevent CPU overuse

                # Wait for remaining tasks
                if tasks:
                    await asyncio.wait(tasks)

        except KeyboardInterrupt:
            logger.info("Interrupted by user. Cleaning up...")
            self.stop_event.set()
            # Cancel remaining tasks
            for task in tasks:
                task.cancel()

        finally:
            # Close progress bars
            main_pbar.close()
            download_pbar.close()

            # Final statistics
            duration = time.time() - self.start_time
            success_count = len(self.successful_downloads)
            failed_count = len(self.failed_urls)
            total_count = success_count + failed_count

            if total_count > 0:
                success_rate = (success_count / total_count) * 100
                speed = success_count / duration

                logger.info(f"Download completed:")
                logger.info(f"- Successfully downloaded: {success_count} images")
                logger.info(f"- Failed downloads: {failed_count}")
                logger.info(f"- Success rate: {success_rate:.1f}%")
                logger.info(f"- Average speed: {speed:.1f} images/second")
                logger.info(f"- Total time: {duration:.1f} seconds")

    async def run(self):
        """Main execution method"""
        self.start_time = time.time()

        try:
            # Download CSV files
            await self.download_csv_files()

            # Load and filter URLs
            self.load_image_urls()

            # Download images
            await self.download_images_async()

        except Exception as e:
            logger.error(f"Error in main execution: {str(e)}")
            raise

async def open_images_downloader():
      downloader = OpenImagesDownloader()
      await downloader.run()

def run_open_image_downloader():
    """Run the async main function"""
    if 'IPython' in sys.modules:
        # Running in IPython/Jupyter
        import nest_asyncio
        nest_asyncio.apply()

    loop = asyncio.get_event_loop()
    try:
        loop.run_until_complete(open_images_downloader())
    except KeyboardInterrupt:
        logger.info("Interrupted by user")
    finally:
        loop.close()

class ImageRepair:
    def __init__(self, image_path):
        self.image_path = image_path
        self.original_image = None
        self.repaired_image = None
        self.diagnosis = {}

    def exists(self):
        """Check if file exists"""
        return os.path.exists(self.image_path)

    def is_empty(self):
        """Check if file is empty"""
        return os.path.getsize(self.image_path) == 0

    def get_image_type(self):
        """Get image type"""
        return imghdr.what(self.image_path)

    def is_valid_image(self):
        """Check if image is valid"""
        try:
            with Image.open(self.image_path) as img:
                img.verify()
                return True
        except Exception as e:
            # warnings.warn(str(e))
            return False

    def get_image_info(self):
        """Get image info"""
        try:
            with Image.open(self.image_path) as img:
                return {
                    "format": img.format,
                    "mode": img.mode,
                    "size": img.size,
                }
        except Exception as e:
            # warnings.warn(str(e))
            return {}

    def diagnose(self):
        """Diagnose image issues"""
        self.diagnosis = {
            "exists": self.exists(),
            "is_valid_image": self.is_valid_image(),
            "format": None,
            "mode": None,
            "size": None,
            "issues": [],
        }

        if not self.diagnosis["exists"]:
            self.diagnosis["issues"].append("File does not exist")
            self.diagnosis["can_be_repaired"] = False
            return self.diagnosis

        if self.is_empty():
            self.diagnosis["issues"].append("File is empty (0 bytes)")
            self.diagnosis["can_be_repaired"] = False
            return self.diagnosis

        img_type = self.get_image_type()
        if img_type is None:
            self.diagnosis["issues"].append("File is not recognized as a valid image format")

        img_info = self.get_image_info()
        self.diagnosis["format"] = img_info.get("format")
        self.diagnosis["mode"] = img_info.get("mode")
        self.diagnosis["size"] = img_info.get("size")

        if 0 in self.diagnosis["size"]:
            self.diagnosis["issues"].append("Image has invalid dimensions")

        if not self.diagnosis["is_valid_image"]:
            self.diagnosis["issues"].append("Image verification failed")

        self.diagnosis["can_be_repaired"] = len(self.diagnosis["issues"]) == 0
        return self.diagnosis

class NSFWImageDownloader:
    def __init__(self,
                 data_dir: str = RAW_DATA_DIR,
                 output_dir: str = NSFW_IMAGES_DIR,
                 max_images: int = UNDETERMINED_DATASET_IMAGES,
                 max_workers: int = 50,
                 chunk_size: int = 1024 * 1024,
                 retry_attempts: int = 3):

        self.data_dir = data_dir
        self.output_dir = output_dir
        self.max_images = max_images
        self.max_workers = max_workers
        self.chunk_size = chunk_size
        self.retry_attempts = retry_attempts
        self.download_count = 0
        self.failed_downloads: Set[str] = set()
        self.successful_downloads: Set[str] = set()
        self.download_lock = threading.Lock()
        self.counter_lock = threading.Lock()
        self.stop_event = threading.Event()
        self.url_queue = asyncio.Queue()
        self.start_time = time.time()
        self.url_cache = set()

        # Progress bars
        self.main_pbar = None
        self.download_pbar = None

        # Create directories if they don't exist
        os.makedirs(output_dir, exist_ok=True)

        # Initialize session
        self.session = self._create_session()

        # Semaphore for concurrent downloads
        self.semaphore = asyncio.Semaphore(max_workers)

    @staticmethod
    def _create_session():
        """Create an optimized requests session with retry strategy"""
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=0.1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=100,
            pool_maxsize=100,
            pool_block=False
        )
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        return session

    @staticmethod
    def find_txt_files(directory):
        """Recursively find all .txt files in directory and subdirectories"""
        txt_files = [os.path.join(root, file)
                     for root, _, files in os.walk(directory)
                     for file in files if file.endswith(".txt")
                     ]

        return sorted(txt_files)

    @staticmethod
    async def verify_image_corruption(file_path: str) -> bool:
        """Verify image and attempt repair if corrupted. Delete if unfixable."""
        try:
            repair = ImageRepair(file_path)
            diagnosis = repair.diagnose()

            if diagnosis["is_valid_image"]:
                return True

            # If we get here, image is corrupted and couldn't be fixed
            if os.path.exists(file_path):
                os.remove(file_path)
                # logger.warning(f"Deleted corrupted image that couldn't be fixed: {file_path}")
            return False

        except Exception as e:
            # logger.error(f"Error during image verification/repair: {str(e)}")
            if os.path.exists(file_path):
                os.remove(file_path)
                # logger.warning(f"Deleted image due to verification error: {file_path}")
            return False

    @staticmethod
    async def verify_image(image_data: bytes, file_path: str) -> bool:
        """Verify image integrity using multiple checks"""
        try:
            # Quick memory check with PIL
            try:
                img = Image.open(io.BytesIO(image_data))
                img.verify()
                img.close()

                # Reopen for size check
                img = Image.open(io.BytesIO(image_data))
                if img.size[0] < 10 or img.size[1] < 10:
                    return False

                # Check for corrupted or empty images
                if img.getbbox() is None:
                    return False

                img.close()
            except (SystemError, IOError):
                return False

            # Save temp file for thorough verification
            temp_path = f"{file_path}.temp"
            async with aiofiles.open(temp_path, 'wb') as f:
                await f.write(image_data)

            # Use ImageRepair for thorough verification
            repair = ImageRepair(temp_path)
            diagnosis = repair.diagnose()

            if os.path.exists(temp_path):
                os.remove(temp_path)

            return diagnosis["is_valid_image"] and not diagnosis["issues"]
        except Exception as e:
            # logger.error(f"Image verification failed: {str(e)}")
            return False

    async def _download_with_aiohttp(self, url: str, filename: str, session: aiohttp.ClientSession) -> bool:
        """Optimized asynchronous download using aiohttp with chunked transfer"""
        try:
            timeout = aiohttp.ClientTimeout(total=30)
            async with session.get(url, timeout=timeout) as response:
                if response.status == 200:
                    async with aiofiles.open(filename, 'wb') as f:
                        while True:
                            chunk = await response.content.read(self.chunk_size)
                            if not chunk:
                                break
                            await f.write(chunk)
                    return True
        except Exception as e:
            # logger.error(f"Download error for {url}: {str(e)}")
            return False

    async def _download_image(self, url: str, session: aiohttp.ClientSession, image_number: int) -> Tuple[bool, str]:
        """Download image with optimized async operations"""
        if self.stop_event.is_set() or self.download_count >= self.max_images:
            return False, ""

        async with self.semaphore:
            for attempt in range(self.retry_attempts):
                try:
                    filename = os.path.join(self.output_dir, f"image{image_number}.jpg")

                    # Skip if file exists and is valid
                    if os.path.exists(filename):
                        return True, filename

                    # Download with optimized async client
                    success = await self._download_with_aiohttp(url, filename, session)
                    if not success:
                        continue

                    return True, filename

                except Exception as e:
                    # logger.error(f"Attempt {attempt + 1} failed for image {image_number}: {str(e)}")
                    if os.path.exists(filename):
                        os.remove(filename)
                    await asyncio.sleep(1)

            return False, ""

    def _print_tqdm(self):
        """Initialize progress bars for download tracking"""
        # Clear any existing progress bars
        if self.main_pbar:
            self.main_pbar.close()
        if self.download_pbar:
            self.download_pbar.close()

        progress_format = "{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        self.main_pbar = tqdm(
            total=self.max_images,
            desc="Total Progress",
            unit="img",
            position=0,
            bar_format=progress_format,
            file=tqdm_out,
            dynamic_ncols=True
        )
        self.download_pbar = tqdm(
            total=self.max_images,
            desc="Downloaded   ",
            unit="img",
            position=1,
            bar_format=progress_format,
            file=tqdm_out,
            dynamic_ncols=True
        )

    async def _verify_and_repair_image(self, filename: str) -> bool:
        """Verify and repair downloaded image"""
        try:
            if await self.verify_image_corruption(filename):
                return True

            repair = ImageRepair(filename)
            diagnosis = repair.diagnose()

            if not diagnosis["is_valid_image"]:
                if os.path.exists(filename):
                    os.remove(filename)
                return False

            return True

        except Exception as e:
            # logger.error(f"Verification/repair failed for image {filename}: {str(e)}")
            if os.path.exists(filename):
                os.remove(filename)
            return False

    async def download_image(self, url: str, session: aiohttp.ClientSession, image_number: int) -> bool:
        """Download and verify a single image with optimized async operations"""
        success, filename = await self._download_image(url, session, image_number)
        if not success:
            return False

        if not await self._verify_and_repair_image(filename):
            return False

        with self.counter_lock:
            self.download_count += 1
            if self.download_pbar:
                self.download_pbar.update(1)

        return True

    @staticmethod
    async def _load_urls_from_file(file_path: str) -> List[str]:
        """Optimized async file reading for a single file"""
        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = await f.read()
                return [url.strip() for url in content.split('\n') if url.strip()]
        except Exception as e:
            # logger.error(f"Error reading file {file_path}: {str(e)}")
            return []

    async def _process_urls_batch(self, urls_batch: List[str]):
        """Process URLs in batches for efficiency"""
        if self.download_count >= self.max_images:
            return

        unique_urls = set(urls_batch) - self.url_cache
        self.url_cache.update(unique_urls)

        for url in unique_urls:
            if self.download_count >= self.max_images:
                break
            await self.url_queue.put(url)
    async def _load_urls_from_all_files(self, data_dir: str) -> List[str]:
        """Load URLs from all text files in directory with optimized async file reading"""
        try:
            txt_files = self.find_txt_files(data_dir)
            if not txt_files:
                # logger.error("No .txt files found!")
                return []

            total_urls = 0
            total_files = 0
            urls_batch = []
            tasks = []

            for file_path in txt_files:
                tasks.append(self._load_urls_from_file(file_path))
                total_files += 1
                if total_files % 10 == 0:
                    # logger.debug(f"Processing {total_files}/{len(txt_files)} files")
                    pass

            results = await asyncio.gather(*tasks, return_exceptions=True)
            for result in results:
                if isinstance(result, Exception):
                    # logger.error(f"Error reading file: {str(result)}")
                    pass
                else:
                    urls_batch.extend(result)

            return urls_batch

        except Exception as e:
            # logger.error(f"Error loading URLs: {str(e)}")
            pass

    async def _process_urls(self, urls_batch: List[str]):
        """Process URLs in batches for efficiency"""
        if self.download_count >= self.max_images:
            return

        unique_urls = set(urls_batch) - self.url_cache
        self.url_cache.update(unique_urls)

        for url in unique_urls:
            if self.download_count >= self.max_images:
                break
            await self.url_queue.put(url)

    async def load_urls_from_files(self):
        """Load URLs from text files with optimized async file reading"""
        try:
            data_dir = os.path.join(self.data_dir, 'raw_data')
            if not os.path.exists(data_dir):
                # logger.error(f"Data directory not found: {data_dir}")
                return

            urls_batch = await self._load_urls_from_all_files(data_dir)
            await self._process_urls(urls_batch)

        except Exception as e:
            # logger.error(f"Error loading URLs: {str(e)}")
            pass

    async def download_worker(self, worker_id: int, session: aiohttp.ClientSession):
        """Optimized worker for processing URLs from the queue"""
        while not self.stop_event.is_set():
            try:
                if self.download_count >= self.max_images:
                    break

                url = await self.url_queue.get()
                current_count = self.download_count + 1

                # Skip if URL is already processed
                url_hash = hashlib.md5(url.encode()).hexdigest()
                if url_hash in self.successful_downloads:
                    self.url_queue.task_done()
                    continue

                success = await self.download_image(url, session, current_count)

                if success:
                    self.successful_downloads.add(url_hash)
                else:
                    self.failed_downloads.add(url)

                self.url_queue.task_done()

                # Rate limiting
                await asyncio.sleep(0.1)

            except asyncio.QueueEmpty:
                break
            except Exception as e:
                # logger.error(f"Worker {worker_id} error: {str(e)}")
                pass

    async def run(self):
        """Main execution function with optimized async operations"""
        try:
            # Initialize progress bars
            self._print_tqdm()

            # Optimized connection pooling
            connector = aiohttp.TCPConnector(
                limit=self.max_workers,
                force_close=True,
                enable_cleanup_closed=True
            )

            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'image/*',
                    'Connection': 'keep-alive'
                }
            ) as session:
                # Load URLs first
                await self.load_urls_from_files()

                # Create and run workers
                workers = []
                for i in range(self.max_workers):
                    worker = asyncio.create_task(self.download_worker(i, session))
                    workers.append(worker)

                # Wait for all workers to complete
                await asyncio.gather(*workers)

                # Wait for queue to be empty
                await self.url_queue.join()

        except Exception as e:
            # logger.error(f"Error in main execution: {str(e)}")
            pass
        finally:
            # Close progress bars
            if self.main_pbar:
                self.main_pbar.close()
            if self.download_pbar:
                self.download_pbar.close()
            self.print_summary()

    def print_summary(self):
        """Print detailed download summary to log file only"""
        elapsed_time = time.time() - self.start_time
        print(f"\nDownload Summary:")
        print(f"Total time: {elapsed_time:.2f} seconds")
        print(f"Successfully downloaded: {len(self.successful_downloads)} images")
        print(f"Failed downloads: {len(self.failed_downloads)} URLs")
        print(f"Average download rate: {len(self.successful_downloads)/elapsed_time:.2f} images/second")

async def nsfw_image_downloader():
      downloader = NSFWImageDownloader()
      await downloader.run()

def run_nsfw_image_downloader():
    try:
        # Try to get the current event loop
        loop = asyncio.get_event_loop()

        # Check if we're in Jupyter/Colab environment
        if loop.is_running():
            # We're in Jupyter/Colab, use nest_asyncio
            try:
                import nest_asyncio
                nest_asyncio.apply()
                loop.run_until_complete(nsfw_image_downloader())
            except ImportError:
                print("Please install nest_asyncio: pip install nest_asyncio")
                return
        else:
            # Regular Python environment
            loop.run_until_complete(main())

    except RuntimeError as e:
        if "no current event loop" in str(e):
            # No event loop exists, create new one
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(main())
    finally:
        with contextlib.suppress(Exception):
            loop.close()

class ImageSimilarityDetector:
    def __init__(self):
        # Load pre-trained ResNet50 model without top classification layer
        self.model = ResNet50(weights='imagenet', include_top=False, pooling='avg')

    def load_and_preprocess_image(self, image_path):
        # Load and preprocess image
        img = image.load_img(image_path, target_size=(224, 224))
        x = image.img_to_array(img)
        x = np.expand_dims(x, axis=0)
        x = preprocess_input(x)
        return x

    def extract_features(self, image_path):
        # Extract features using ResNet50
        try:
            img = self.load_and_preprocess_image(image_path)
            features = self.model.predict(img)
            return features
        except Exception as e:
            print(f"Error processing {image_path}: {str(e)}")
            return None

    def find_similar_images(self, directory, similarity_threshold=0.95):
        # Get all image files
        image_files = []
        for root, _, files in os.walk(directory):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):
                    image_files.append(os.path.join(root, file))

        if len(image_files) < 2:
            print("Not enough images found for comparison")
            return

        # Extract features for all images
        features_dict = {}
        for img_path in image_files:
            features = self.extract_features(img_path)
            if features is not None:
                features_dict[img_path] = features

        # Find similar images
        similar_groups = []
        processed = set()

        for img1_path, feat1 in features_dict.items():
            if img1_path in processed:
                continue

            current_group = [img1_path]

            for img2_path, feat2 in features_dict.items():
                if img1_path != img2_path and img2_path not in processed:
                    similarity = cosine_similarity(feat1, feat2)[0][0]
                    if similarity > similarity_threshold:
                        current_group.append(img2_path)
                        processed.add(img2_path)

            if len(current_group) > 1:
                similar_groups.append(current_group)
            processed.add(img1_path)

        return similar_groups

    def delete_similar_images(self, similar_groups, keep_first=True):
        """Delete similar images, keeping either the first or none based on keep_first parameter"""
        deleted_count = 0
        for group in similar_groups:
            print(f"\nFound similar images:")
            for img_path in group:
                print(f"- {img_path}")

            start_idx = 1 if keep_first else 0
            for img_path in group[start_idx:]:
                try:
                    os.remove(img_path)
                    print(f"Deleted: {img_path}")
                    deleted_count += 1
                except Exception as e:
                    print(f"Error deleting {img_path}: {str(e)}")

        return deleted_count

def run_image_similarity_detector():
    # Initialize detector
    detector = ImageSimilarityDetector()

    # Get directory path from user
    directory = NSFW_IMAGES_DIR

    if not os.path.exists(directory):
        print("Directory does not exist!")
        return

    # Find similar images
    print("\nAnalyzing images...")
    similar_groups = detector.find_similar_images(directory)

    if not similar_groups:
        print("No similar images found!")
        return

    # Ask user for confirmation before deletion
    total_similar = sum(len(group) - 1 for group in similar_groups)
    print(f"\nFound {total_similar} similar images.")

    keep_original = 'no'
    confirm = 'yes'

    if confirm == 'yes':
        deleted = detector.delete_similar_images(similar_groups, keep_first=keep_original)
        print(f"\nDeleted {deleted} similar images.")
    else:
        print("Operation cancelled.")

def reorder_image_files(directory=NSFW_IMAGES_DIR):
    """
    Reorders image files in a directory sequentially starting from 1,
    preserving the original file extensions.
    """
    # Get all files in the directory, ignoring subdirectories
    files = [
        f for f in os.listdir(directory)
        if os.path.isfile(os.path.join(directory, f))
    ]

    # Sort files to ensure a consistent renaming order
    files = natsorted(files)

    # Temporary renaming to avoid conflicts
    temp_names = {}
    for i, f in enumerate(files, start=1):
        old_path = os.path.join(directory, f)
        temp_name = f"temp_{i:05d}{os.path.splitext(f)[1]}"  # Temporary unique name
        temp_path = os.path.join(directory, temp_name)
        os.rename(old_path, temp_path)
        temp_names[temp_name] = f  # Map temp names back to original for logging

    # Rename to final sequence
    for i, temp_name in enumerate(natsorted(temp_names), start=1):
        temp_path = os.path.join(directory, temp_name)
        new_name = f"image{i}.jpg"  # Final name
        new_path = os.path.join(directory, new_name)
        os.rename(temp_path, new_path)
        logger.info(f"Renamed: {temp_names[temp_name]} -> {new_name}")

class DatasetOptimizer:
    def __init__(self, target_dir=NSFW_IMAGES_DIR, target_count=DATASET_IMAGES):
        self.target_dir = target_dir
        self.target_count = target_count
        self.similarity_detector = ImageSimilarityDetector()
        self.downloader = NSFWImageDownloader(output_dir=target_dir)

    def remove_low_quality_images(self):
        """Remove images with low quality (low contrast, shady, low colors)"""
        removed_count = 0
        for filename in os.listdir(self.target_dir):
            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue

            filepath = os.path.join(self.target_dir, filename)
            try:
                img = cv2.imread(filepath)
                if img is None:
                    os.remove(filepath)
                    removed_count += 1
                    continue

                # Check contrast
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                contrast = cv2.meanStdDev(gray)[1][0][0]

                # Check color variety
                unique_colors = len(np.unique(img.reshape(-1, 3), axis=0))

                # Check brightness
                brightness = np.mean(gray)

                # Remove if image is low quality
                if contrast < 25 or unique_colors < 1000 or brightness < 30 or brightness > 225:
                    os.remove(filepath)
                    removed_count += 1

            except Exception as e:
                logger.error(f"Error processing {filename}: {str(e)}")

        return removed_count

    def optimize_dataset(self):
        """Main method to optimize the dataset"""
        while True:
            current_count = len([f for f in os.listdir(self.target_dir)
                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])

            logger.info(f"Current image count: {current_count}")

            # Find and remove similar images
            similar_groups = self.similarity_detector.find_similar_images(self.target_dir, similarity_threshold=0.95)
            if similar_groups:
                deleted_count = self.similarity_detector.delete_similar_images(similar_groups, keep_first=True)
                logger.info(f"Removed {deleted_count} similar images")

            # Remove low quality images
            removed_count = self.remove_low_quality_images()
            logger.info(f"Removed {removed_count} low quality images")

            # Check if we need more images
            current_count = len([f for f in os.listdir(self.target_dir)
                               if f.lower().endswith(('.png', '.jpg', '.jpeg'))])

            if current_count >= self.target_count:
                # If we have more than needed, remove excess images
                files = [f for f in os.listdir(self.target_dir)
                        if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
                files.sort()  # Sort to ensure consistent removal
                excess = current_count - self.target_count
                for f in files[:excess]:
                    os.remove(os.path.join(self.target_dir, f))
                break
            else:
                # Download more images
                additional_needed = self.target_count - current_count + 5000  # Download extra to account for removals
                self.downloader.max_images = additional_needed
                self.downloader.run()

        # Final reordering
        reorder_image_files(self.target_dir)
        logger.info("Dataset optimization completed")

def run_optimize_dataset():
    optimizer = DatasetOptimizer(target_dir=NSFW_IMAGES_DIR, target_count=DATASET_IMAGES)
    optimizer.optimize_dataset()

class LabelGenerator:
    def __init__(self, base_dir=BASE_DIR):
        self.base_dir = base_dir
        self.classes = {
            'NSFW': 0,
            'SFW': 1
        }

    def create_directory_structure(self):
        for class_name in self.classes.keys():
            labels_dir = os.path.join(self.base_dir, class_name, 'labels')
            os.makedirs(labels_dir, exist_ok=True)

    def generate_label_file(self, image_path, class_name):
        image_name = os.path.splitext(os.path.basename(image_path))[0]
        label_dir = os.path.join(self.base_dir, class_name, 'labels')
        label_path = os.path.join(label_dir, f"{image_name}.txt")

        with open(label_path, 'w') as f:
            f.write(f"{self.classes[class_name]}")

    def generate_all_labels(self):
        self.create_directory_structure()

        for class_name in self.classes.keys():
            images_dir = os.path.join(self.base_dir, class_name, 'images')
            if not os.path.exists(images_dir):
                logger.warning(f"Directory not found: {images_dir}")
                continue

            logger.info(f"Generating labels for {class_name} images...")
            image_files = [f for f in os.listdir(images_dir)
                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

            for image_file in tqdm(image_files, desc=f"Processing {class_name}"):
                image_path = os.path.join(images_dir, image_file)
                self.generate_label_file(image_path, class_name)

        logger.info("Label generation completed")

    def verify_labels(self):
        missing_labels = []

        for class_name in self.classes.keys():
            images_dir = os.path.join(self.base_dir, class_name, 'images')
            labels_dir = os.path.join(self.base_dir, class_name, 'labels')

            if not os.path.exists(images_dir) or not os.path.exists(labels_dir):
                continue

            image_files = set([os.path.splitext(f)[0] for f in os.listdir(images_dir)
                             if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
            label_files = set([os.path.splitext(f)[0] for f in os.listdir(labels_dir)
                             if f.lower().endswith('.txt')])

            missing = image_files - label_files
            if missing:
                missing_labels.extend([f"{class_name}/{f}" for f in missing])

        if missing_labels:
            logger.warning("Missing labels for the following images:")
            for img in missing_labels:
                logger.warning(f"- {img}")
        else:
            logger.info("All images have corresponding label files")

        return len(missing_labels) == 0

def run_labels_generator():
    generator = LabelGenerator(base_dir=BASE_DIR)
    generator.generate_all_labels()
    generator.verify_labels()

class DatasetVerifier:
    def __init__(self, base_dir=BASE_DIR, expected_count=DATASET_IMAGES):
        self.base_dir = base_dir
        self.expected_count = expected_count
        self.classes = ['NSFW', 'SFW']

    def verify_file_sequence(self, directory):
        files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]
        expected_files = {f"image{i}.jpg" for i in range(1, self.expected_count + 1)}
        missing_files = expected_files - set(files)
        extra_files = set(files) - expected_files
        return missing_files, extra_files

    def verify_image_label_pairs(self, class_name):
        images_dir = os.path.join(self.base_dir, class_name, 'images')
        labels_dir = os.path.join(self.base_dir, class_name, 'labels')

        if not os.path.exists(images_dir) or not os.path.exists(labels_dir):
            logger.error(f"Directory not found for {class_name}")
            return False

        # Check sequences
        missing_images, extra_images = self.verify_file_sequence(images_dir)
        expected_labels = {f"image{i}.txt" for i in range(1, self.expected_count + 1)}
        existing_labels = set(f for f in os.listdir(labels_dir) if f.endswith('.txt'))
        missing_labels = expected_labels - existing_labels
        extra_labels = existing_labels - expected_labels

        # Check content
        invalid_labels = []
        invalid_images = []
        expected_class_id = '0' if class_name == 'NSFW' else '1'

        # Verify labels
        for label_file in existing_labels:
            if label_file in expected_labels:
                label_path = os.path.join(labels_dir, label_file)
                try:
                    with open(label_path, 'r') as f:
                        if f.read().strip() != expected_class_id:
                            invalid_labels.append(label_file)
                except:
                    invalid_labels.append(label_file)

        # Verify images
        for img_file in os.listdir(images_dir):
            if img_file.endswith('.jpg'):
                img_path = os.path.join(images_dir, img_file)
                try:
                    with Image.open(img_path) as img:
                        img.verify()
                except:
                    invalid_images.append(img_file)

        # Report issues
        if missing_images:
            logger.error(f"{class_name} missing images: {sorted(list(missing_images))[:10]}")
        if extra_images:
            logger.error(f"{class_name} extra images: {sorted(list(extra_images))[:10]}")
        if missing_labels:
            logger.error(f"{class_name} missing labels: {sorted(list(missing_labels))[:10]}")
        if extra_labels:
            logger.error(f"{class_name} extra labels: {sorted(list(extra_labels))[:10]}")
        if invalid_labels:
            logger.error(f"{class_name} invalid labels: {sorted(invalid_labels)[:10]}")
        if invalid_images:
            logger.error(f"{class_name} corrupted images: {sorted(invalid_images)[:10]}")

        return not (missing_images or extra_images or missing_labels or
                   extra_labels or invalid_labels or invalid_images)

    def verify_dataset(self):
        logger.info("Starting dataset verification...")
        all_valid = True

        for class_name in self.classes:
            logger.info(f"Verifying {class_name} dataset...")
            if not self.verify_image_label_pairs(class_name):
                all_valid = False
                logger.error(f"{class_name} dataset verification failed")
            else:
                logger.info(f"{class_name} dataset verification passed")

        if all_valid:
            logger.info(f"Dataset verification completed successfully!")
            logger.info(f"Both classes have exactly {self.expected_count} valid images and labels")
        else:
            logger.error("Dataset verification failed. Please check the errors above")

        return all_valid

def run_verify_dataset():
    verifier = DatasetVerifier()
    verifier.verify_dataset()

class DatasetSplitter:
    def __init__(self, base_dir=BASE_DIR, output_dir=DATA_SPLIT_OUTPUT_DIR,
                 train_size=TRAIN_SIZE, val_size=VAL_SIZE, test_size=TEST_SIZE, random_state=RANDOM_STATE):
        self.base_dir = base_dir
        self.output_dir = output_dir
        self.train_size = train_size
        self.val_size = val_size
        self.test_size = test_size
        self.random_state = random_state
        self.classes = ['NSFW', 'SFW']

    def create_directory_structure(self):
        """Create the directory structure for train/val/test splits"""
        splits = ['train', 'val', 'test']
        for split in splits:
            for class_name in self.classes:
                for subdir in ['images', 'labels']:
                    path = os.path.join(self.output_dir, split, class_name, subdir)
                    os.makedirs(path, exist_ok=True)

    def get_image_label_pairs(self, class_name):
        """Get list of image-label pairs for a class"""
        images_dir = os.path.join(self.base_dir, class_name, 'images')
        labels_dir = os.path.join(self.base_dir, class_name, 'labels')

        image_files = sorted([f for f in os.listdir(images_dir)
                            if f.lower().endswith('.jpg')])
        pairs = []

        for img_file in image_files:
            base_name = os.path.splitext(img_file)[0]
            label_file = f"{base_name}.txt"
            if os.path.exists(os.path.join(labels_dir, label_file)):
                pairs.append((img_file, label_file))

        return pairs

    def copy_files(self, files, src_class, split):
        """Copy image-label pairs to their respective split directories"""
        for img_file, label_file in tqdm(files, desc=f"Copying {split} {src_class} files"):
            # Copy image
            src_img = os.path.join(self.base_dir, src_class, 'images', img_file)
            dst_img = os.path.join(self.output_dir, split, src_class, 'images', img_file)
            shutil.copy2(src_img, dst_img)

            # Copy label
            src_label = os.path.join(self.base_dir, src_class, 'labels', label_file)
            dst_label = os.path.join(self.output_dir, split, src_class, 'labels', label_file)
            shutil.copy2(src_label, dst_label)

    def split_dataset(self):
        """Split the dataset into train/val/test sets"""
        logger.info("Starting dataset splitting...")
        self.create_directory_structure()

        for class_name in self.classes:
            logger.info(f"Processing {class_name} class...")
            pairs = self.get_image_label_pairs(class_name)

            if not pairs:
                logger.error(f"No valid image-label pairs found for {class_name}")
                continue

            # First split: train vs (val+test)
            train_pairs, temp_pairs = train_test_split(
                pairs,
                train_size=self.train_size,
                random_state=self.random_state,
                shuffle=True
            )

            # Second split: val vs test from remaining data
            relative_val_size = self.val_size / (self.val_size + self.test_size)
            val_pairs, test_pairs = train_test_split(
                temp_pairs,
                train_size=relative_val_size,
                random_state=self.random_state,
                shuffle=True
            )

            # Copy files to their respective directories
            self.copy_files(train_pairs, class_name, 'train')
            self.copy_files(val_pairs, class_name, 'val')
            self.copy_files(test_pairs, class_name, 'test')

            # Log split sizes
            logger.info(f"{class_name} split sizes:")
            logger.info(f"  Train: {len(train_pairs)} files")
            logger.info(f"  Validation: {len(val_pairs)} files")
            logger.info(f"  Test: {len(test_pairs)} files")

        logger.info("Dataset splitting completed!")

        # Create a summary file
        summary_path = os.path.join(self.output_dir, 'dataset_summary.txt')
        with open(summary_path, 'w') as f:
            f.write("Dataset Split Summary\n")
            f.write("===================\n\n")
            f.write(f"Split Ratios:\n")
            f.write(f"  Train: {self.train_size*100}%\n")
            f.write(f"  Validation: {self.val_size*100}%\n")
            f.write(f"  Test: {self.test_size*100}%\n\n")

            for class_name in self.classes:
                train_count = len(os.listdir(os.path.join(self.output_dir, 'train', class_name, 'images')))
                val_count = len(os.listdir(os.path.join(self.output_dir, 'val', class_name, 'images')))
                test_count = len(os.listdir(os.path.join(self.output_dir, 'test', class_name, 'images')))

                f.write(f"{class_name} Class:\n")
                f.write(f"  Train: {train_count} images\n")
                f.write(f"  Validation: {val_count} images\n")
                f.write(f"  Test: {test_count} images\n\n")

def run_split_dataset():
    splitter = DatasetSplitter()
    splitter.split_dataset()

class DataAugmentor:
    def __init__(self, split_dir=DATA_SPLIT_OUTPUT_DIR, output_dir=DATA_AUGMENTATION_OUTPUT_DIR,
                 target_per_class=DATASET_IMAGES, random_state=RANDOM_STATE):
        self.split_dir = split_dir
        self.output_dir = output_dir
        self.target_per_class = target_per_class
        self.random_state = random_state
        self.classes = ['NSFW', 'SFW']

        # Initialize augmentation parameters
        self.brightness_range = (0.8, 1.2)
        self.contrast_range = (0.8, 1.2)
        self.rotation_range = (-15, 15)
        self.scale_range = (0.9, 1.1)

    def create_directory_structure(self):
        """Create augmented dataset directory structure"""
        for split in ['train', 'val', 'test']:
            for class_name in self.classes:
                for subdir in ['images', 'labels']:
                    path = os.path.join(self.output_dir, split, class_name, subdir)
                    os.makedirs(path, exist_ok=True)

    def apply_augmentation(self, image):
        """Apply random augmentations to an image"""
        # Convert to PIL Image if needed
        if isinstance(image, np.ndarray):
            image = Image.fromarray(image)

        # Random brightness
        enhancer = ImageEnhance.Brightness(image)
        brightness_factor = random.uniform(*self.brightness_range)
        image = enhancer.enhance(brightness_factor)

        # Random contrast
        enhancer = ImageEnhance.Contrast(image)
        contrast_factor = random.uniform(*self.contrast_range)
        image = enhancer.enhance(contrast_factor)

        # Random rotation
        rotation_angle = random.uniform(*self.rotation_range)
        image = image.rotate(rotation_angle, expand=False)

        # Random scale
        scale_factor = random.uniform(*self.scale_range)
        new_size = tuple(int(dim * scale_factor) for dim in image.size)
        image = image.resize(new_size, Image.Resampling.LANCZOS)

        # Resize back to original size if changed
        if new_size != image.size:
            image = image.resize(image.size, Image.Resampling.LANCZOS)

        return image

    def augment_class(self, split, class_name):
        """Augment images for a specific class in a split"""
        src_img_dir = os.path.join(self.split_dir, split, class_name, 'images')
        src_label_dir = os.path.join(self.split_dir, split, class_name, 'labels')

        dst_img_dir = os.path.join(self.output_dir, split, class_name, 'images')
        dst_label_dir = os.path.join(self.output_dir, split, class_name, 'labels')

        # Get existing images
        images = [f for f in os.listdir(src_img_dir)
                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

        if not images:
            logger.error(f"No images found in {src_img_dir}")
            return

        # First, copy original files
        for img_file in images:
            # Copy image
            shutil.copy2(
                os.path.join(src_img_dir, img_file),
                os.path.join(dst_img_dir, img_file)
            )
            # Copy label
            label_file = os.path.splitext(img_file)[0] + '.txt'
            shutil.copy2(
                os.path.join(src_label_dir, label_file),
                os.path.join(dst_label_dir, label_file)
            )

        # Only augment training data
        if split != 'train':
            return

        current_count = len(images)
        augmentations_needed = max(0, self.target_per_class - current_count)

        if augmentations_needed == 0:
            logger.info(f"No augmentation needed for {class_name} in {split}")
            return

        logger.info(f"Generating {augmentations_needed} augmented images for {class_name} in {split}")

        # Generate augmented images
        pbar = tqdm(total=augmentations_needed, desc=f"Augmenting {class_name}")
        aug_count = 0
        while aug_count < augmentations_needed:
            # Randomly select an image to augment
            img_file = random.choice(images)
            img_path = os.path.join(src_img_dir, img_file)

            try:
                # Load and augment image
                with Image.open(img_path) as img:
                    augmented = self.apply_augmentation(img)

                    # Save augmented image
                    aug_name = f"aug_{aug_count}_{img_file}"
                    aug_path = os.path.join(dst_img_dir, aug_name)
                    augmented.save(aug_path, quality=95)

                    # Copy corresponding label with new name
                    src_label = os.path.join(src_label_dir, os.path.splitext(img_file)[0] + '.txt')
                    dst_label = os.path.join(dst_label_dir, f"aug_{aug_count}_{os.path.splitext(img_file)[0]}.txt")
                    shutil.copy2(src_label, dst_label)

                    aug_count += 1
                    pbar.update(1)

            except Exception as e:
                logger.error(f"Error augmenting {img_file}: {str(e)}")
                continue

        pbar.close()

    def augment_dataset(self):
        """Augment the entire dataset"""
        logger.info("Starting dataset augmentation...")
        self.create_directory_structure()

        for split in ['train', 'val', 'test']:
            logger.info(f"Processing {split} split...")
            for class_name in self.classes:
                self.augment_class(split, class_name)

        logger.info("Dataset augmentation completed!")

        # Create summary file
        summary_path = os.path.join(self.output_dir, 'augmentation_summary.txt')
        with open(summary_path, 'w') as f:
            f.write("Dataset Augmentation Summary\n")
            f.write("==========================\n\n")

            for split in ['train', 'val', 'test']:
                f.write(f"{split.upper()} Split:\n")
                for class_name in self.classes:
                    img_dir = os.path.join(self.output_dir, split, class_name, 'images')
                    count = len([f for f in os.listdir(img_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
                    f.write(f"  {class_name}: {count} images\n")
                f.write("\n")

def run_augment_dataset():
    augmentor = DataAugmentor()
    augmentor.augment_dataset()

class YOLOTrainer:
    def __init__(self, data_dir=DATA_AUGMENTATION_OUTPUT_DIR, model_dir=MODEL_DIR,
                 epochs=EPOCHS, imgsz=IMGSIZE, batch_size=BATCH_SIZE):
        self.data_dir = data_dir
        self.model_dir = model_dir
        self.epochs = epochs
        self.imgsz = imgsz
        self.batch_size = batch_size
        os.makedirs(model_dir, exist_ok=True)

    def create_dataset_yaml(self):
        """Create YAML file for YOLO training"""
        yaml_content = {
            'path': os.path.abspath(self.data_dir),  # Dataset root dir
            'train': 'train',  # Train dir (relative to 'path')
            'val': 'val',      # Val dir (relative to 'path')
            'test': 'test',    # Test dir (relative to 'path')

            'names': {
                0: 'NSFW',
                1: 'SFW'
            },

            'nc': 2  # Number of classes
        }

        yaml_path = os.path.join(self.model_dir, 'dataset.yaml')
        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f, sort_keys=False)

        return yaml_path

    def train(self):
        """Train YOLOv8 model"""
        logger.info("Starting YOLO training...")

        # Create dataset YAML
        yaml_path = self.create_dataset_yaml()

        # Initialize YOLO model
        model = YOLO('yolov8n-cls.pt')  # Use YOLO classification model

        # Train the model
        results = model.train(
            data=yaml_path,
            epochs=self.epochs,
            imgsz=self.imgsz,
            batch=self.batch_size,
            device='0',  # Use GPU if available
            project=self.model_dir,
            name='yolo_nsfw_classifier',
            exist_ok=True,
            pretrained=True,
            optimizer='auto',
            verbose=True,
            amp=True,  # Automatic Mixed Precision
            patience=50,  # Early stopping patience
            save=True,  # Save best model
            plots=True  # Generate training plots
        )

        logger.info("Training completed!")
        return results

    def validate(self):
        """Validate the trained model"""
        logger.info("Starting validation...")

        # Load best model
        model_path = os.path.join(self.model_dir, 'yolo_nsfw_classifier', 'weights', 'best.pt')
        if not os.path.exists(model_path):
            logger.error("No trained model found!")
            return None

        model = YOLO(model_path)

        # Run validation
        results = model.val(
            data=os.path.join(self.model_dir, 'dataset.yaml'),
            split='test'  # Use test set for final validation
        )

        logger.info("Validation completed!")
        return results

    def export_model(self, format='onnx'):
        """Export the model to different formats"""
        logger.info(f"Exporting model to {format}...")

        model_path = os.path.join(self.model_dir, 'yolo_nsfw_classifier', 'weights', 'best.pt')
        if not os.path.exists(model_path):
            logger.error("No trained model found!")
            return

        model = YOLO(model_path)
        model.export(format=format)

        logger.info(f"Model exported to {format} format!")

def run_train_yolo():
    trainer = YOLOTrainer()

    results = trainer.train()

    val_results = trainer.validate()

    trainer.export_model(format='onnx')

    return trainer

def main():
    """Main function to run the entire pipeline"""
    try:
        logger.info("Starting the NSFW/SFW Dataset Creation Pipeline...")

        # Step 1: Create dataset
        logger.info("Step 1: Creating datasets...")
        
        # Step 1.1: Create SFW dataset
        logger.info("Step 1.1: Creating SFW dataset...")
        if not run_open_image_downloader():
            raise Exception("Failed to create SFW dataset")
        
        # Step 1.2: Create NSFW dataset
        logger.info("Step 1.2: Creating NSFW dataset...")
        if not run_nsfw_image_downloader():
            raise Exception("Failed to create NSFW dataset")
        
        # Step 2: Optimize dataset
        logger.info("Step 2: Optimizing dataset...")
        if not run_optimize_dataset():
            raise Exception("Failed to optimize dataset")
            
        # Step 3: Generate labels
        logger.info("Step 3: Generating labels...")
        if not run_labels_generator():
            raise Exception("Failed to generate labels")
        
        # Step 4: Verify dataset
        logger.info("Step 4: Verifying dataset...")
        if not run_verify_dataset():
            raise Exception("Failed to verify dataset")
        
        # Step 5: Split dataset
        logger.info("Step 5: Splitting dataset...")
        if not run_split_dataset():
            raise Exception("Failed to split dataset")
        
        # Step 6: Augment dataset
        logger.info("Step 6: Augmenting dataset...")
        if not run_augment_dataset():
            raise Exception("Failed to augment dataset")
        
        # Step 7: Train YOLO model
        logger.info("Step 7: Training YOLO model...")
        if not run_train_yolo():
            raise Exception("Failed to train YOLO model")
            
        logger.info("Pipeline completed successfully!")
        return True
        
    except KeyboardInterrupt:
        logger.warning("\nProcess interrupted by user")
        return False
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        logger.exception("Detailed error traceback:")
        return False

if __name__ == "__main__":
    main()